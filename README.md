# NBA-Rating ML ‚Äì README

## üìä Data, Methods & Sources

| Aspect | Description |
|--------|-------------|
| **Data type** | *Tabular sports data* covering every NBA season **1999 ‚Üí 2023**. <br>‚Ä¢ **Game-level box-scores** (PTS, REB, AST, ¬± ‚Ä¶) <br>‚Ä¢ **Player physiques** (height cm, weight kg, BMI, age, position) <br>‚Ä¢ **Derived stats** per season : `min_per_game`, per-36 rates, shooting efficiency (eFG %, TS %), Estimated Shot Value `esv_mean` <br>‚Ä¢ **Target** : `score_100`, unidimensional rating recentred to Œº = 50, œÉ = 25 |
| **Sources & API endpoints** | [`nba_api`](https://github.com/swar/nba_api) ‚Äì official NBA Stats REST  <br>‚Ä¢ `LeagueGameLog` ‚Üí box-scores  <br>‚Ä¢ `CommonTeamRoster` ‚Üí physiques & position  <br>‚Ä¢ `LeagueDashPlayerStats` ‚Üí shot profile (ESV)  <br>All calls automatised in **`scripts/collect_raw.py`**; parquet caches under `nba_rating/data/raw/`. |
| **Data preparation** | **`prepare_curated.py`** & **`feature_engineering.py`**  <br>1. Aggregate game logs ‚Üí season averages per player  <br>2. Merge physiques, compute BMI & availability (`gp/82`)  <br>3. Add per-36 & efficiency features  <br>4. Output `player_season_YYYY-YY.parquet` in `data/curated/`. |
| **Purpose** | ‚Ä¢ Build a **rating reference** (`score_100`) comparable across eras  <br>‚Ä¢ Train a model predicting **next-season score** (`note_n+1`) to flag breakout/decline trends  <br>‚Ä¢ Feed a future Streamlit dashboard for scouting & fan analytics. |

---

## ü§ñ Machine-Learning Methods & Rationale

| Mod√®le | Pourquoi ce choix ? |
|--------|--------------------|
| **Ridge Regression** | Baseline lin√©aire r√©gularis√©e (L2) : interpr√©table, tr√®s rapide, souvent au plus proche du RMSE mini apr√®s normalisation Z-score. |
| **HistGradientBoostingRegressor** | Boosting √† histogrammes (scikit-learn) : capture non-lin√©arit√©s & interactions, tuning mod√©r√©, robuste aux grandes tables. |
| **LightGBM** | Impl√©mentation GBDT ultra-rapide (CPU/GPU), g√®re nativement missing values & cat√©gories, ‚âà 5 % plus pr√©cis que HGB pour un co√ªt calcul raisonnable. |
| **MLPRegressor** | R√©seau feed-forward (64-128-64) : apprend des patterns non lin√©aires plus fins ; gagne ~0.5 % RMSE sur nos donn√©es pour un temps < 5 s. |
| **Stacking final** | Ensemble Ridge + HGB + LightGBM (+ MLP) avec meta-Ridge : combine forces lin√©aires & non-lin√©aires, ~-2 % RMSE suppl√©mentaire en CV. |

> **Crit√®res de s√©lection** : performance (RMSE / R¬≤, 5-fold CV), robustesse (variance entre folds), temps d‚Äôentra√Ænement/pr√©diction (CI-friendly), interpr√©tabilit√© (coeffs Ridge, permutation importance HGB/LGBM).
