#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
dashboard_data.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Assemble toutes les sources de donnÃ©es nÃ©cessaires au dashboard
MLPlayers **sans lancer Streamlit** et produit un unique
`dashboard_data.parquet` dans `data/curated/`.

Sources fusionnÃ©es
------------------
â€¢ dataset_ml.parquet         â†’Â features & score normalisÃ©  
â€¢ all_seasons_scores.parquet â†’Â score_100 par saison  
â€¢ wins_shares_vorp.parquet   â†’Â WinÂ Shares & VORP  
â€¢ player_clusters.parquet    â†’Â cluster Kâ€‘means / rÃ´le  
â€¢ all_player_gamelogs.parquetâ†’Â noms officiels des joueurs  
â€¢ projections.parquet        â†’Â prÃ©visions de score Ã Â horizonÂ 1Â Ã Â 5  

Usage
-----
```bash
python -m nba_rating.scripts.dashboard_data
```
"""

from __future__ import annotations
import pandas as pd
from pathlib import Path
import pyarrow.parquet as pq

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  1.Â Chemins de base
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE = Path(__file__).resolve().parents[1] / "data"
CURATED = BASE / "curated"

IN_DS   = CURATED / "dataset_ml.parquet"
IN_S100 = CURATED / "all_seasons_scores.parquet"
IN_WS   = CURATED / "wins_shares_vorp.parquet"
IN_CL   = CURATED / "player_clusters.parquet"
IN_LOGS = CURATED / "all_player_gamelogs.parquet"
IN_PROJ = CURATED / "projections.parquet"

OUTFILE = CURATED / "dashboard_data.parquet"


def make_photo_url(player_id: int, size: str = "260x190") -> str:
    """Renvoie lâ€™URL CDN NBA du headâ€‘shot du joueur."""
    return f"https://cdn.nba.com/headshots/nba/latest/{size}/{int(player_id)}.png"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  2.Â Dataset principal
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"ğŸ”„  Chargement      : {IN_DS.name}")
df = pd.read_parquet(IN_DS)  # contient PLAYER_ID, season, score, etc.

# Pour les joins ultÃ©rieurs
id_name_map = (
    df[["PLAYER_ID", "player_name"]].drop_duplicates()
    if "player_name" in df.columns
    else pd.DataFrame(columns=["PLAYER_ID", "player_name"])
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  3.Â Ajout score_100
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if IN_S100.exists():
    print(f"ğŸ”„  Fusion score_100 : {IN_S100.name}")
    s100 = (
        pd.read_parquet(IN_S100)[["PLAYER_ID", "season", "score_100"]]
        .drop_duplicates()
    )
    df = df.merge(s100, on=["PLAYER_ID", "season"], how="left")
else:
    print(f"âš ï¸  {IN_S100.name} manquant â€“ score_100 non ajoutÃ©")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  4.Â Extraction noms officiels
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if IN_LOGS.exists():
    print(f"ğŸ”„  Extraction noms  : {IN_LOGS.name}")
    names = (
        pd.read_parquet(IN_LOGS, columns=["PLAYER_ID", "PLAYER_NAME"])
        .drop_duplicates()
        .rename(columns={"PLAYER_NAME": "player_name_official"})
    )
    df = df.merge(names, on="PLAYER_ID", how="left")
    # PrÃ©fÃ©rence ordreÂ : officiel > existant
    if "player_name" in df.columns:
        df["player_name"] = df["player_name_official"].fillna(df["player_name"])
    else:
        df = df.rename(columns={"player_name_official": "player_name"})
else:
    print(f"âš ï¸  {IN_LOGS.name} manquant â€“ noms non mis Ã Â jour")

# URL photo
df["photo_url"] = df["PLAYER_ID"].apply(make_photo_url)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  5.Â WinÂ SharesÂ /Â VORP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if IN_WS.exists():
    print(f"ğŸ”„  Fusion WS/VORP   : {IN_WS.name}")
    ws = pd.read_parquet(IN_WS).drop_duplicates(subset=["PLAYER_ID", "season"])
    df = df.merge(ws, on=["PLAYER_ID", "season"], how="left")
else:
    print(f"âš ï¸  {IN_WS.name} manquant â€“ WS/VORP non ajoutÃ©s")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  6.Â Clusters
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if IN_CL.exists():
    print(f"ğŸ”„  Fusion clusters  : {IN_CL.name}")
    cl = pd.read_parquet(IN_CL).drop_duplicates(subset=["PLAYER_ID", "season"])
    cl = cl.rename(columns={"player_cluster": "cluster"}) if "player_cluster" in cl.columns else cl
    df = df.merge(cl[["PLAYER_ID", "season", "cluster"]], on=["PLAYER_ID", "season"], how="left")
else:
    print(f"âš ï¸  {IN_CL.name} manquant â€“ clusters non ajoutÃ©s")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  7.Â Projections multiâ€‘horizon
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if IN_PROJ.exists():
    print(f"ğŸ”„  Fusion projections: {IN_PROJ.name}")
    proj = pd.read_parquet(IN_PROJ)

    # Harmoniser les noms de colonnes
    proj = proj.rename(columns=str.lower)  # player_id, player_name, horizon, pred_scoreâ€¦

    # Si player_id absent, lâ€™ajouter via le mapping nom â†” id
    if "player_id" not in proj.columns:
        if not id_name_map.empty:
            proj = proj.merge(
                id_name_map.rename(columns={"PLAYER_ID": "player_id"}),
                left_on="player_name",
                right_on="player_name",
                how="left",
            )
        missing = proj["player_id"].isna().sum()
        if missing:
            print(f"âš ï¸  {missing} projections sans PLAYER_ID (nom non trouvÃ©)")

    # PivotÂ : 1 colonne par horizon (proj_1Â â€¦Â proj_5)
    proj_wide = (
        proj.pivot_table(
            index="player_id",
            columns="horizon",
            values="pred_score",
            aggfunc="first",
        )
        .add_prefix("proj_")
        .reset_index()
    )

    df = df.merge(proj_wide, left_on="PLAYER_ID", right_on="player_id", how="left")
    df = df.drop(columns=["player_id"])
else:
    print(f"âš ï¸  {IN_PROJ.name} manquant â€“ projections non ajoutÃ©es")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  7.bÂ Ã‰quipe principale (team_id)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if IN_LOGS.exists():
    print(f"ğŸ”„  Attribution Ã©quipe principale : {IN_LOGS.name}")
    # On choisit lâ€™Ã©quipe la plus frÃ©quente jouÃ©e par le joueur sur la saison
    # â€‘Â DÃ©tection du nom de colonne saison dans les gamelogsÂ : "SEASON" ou "SEASON_ID"
    # Lecture ultraâ€‘lÃ©gÃ¨re du schÃ©ma (0Â I/O) pour dÃ©tecter les noms de colonnes
    sample_cols = pq.ParquetFile(IN_LOGS).schema.names

    season_src  = "SEASON" if "SEASON" in sample_cols else "SEASON_ID"

    logs_min = pd.read_parquet(
        IN_LOGS,
        columns=["PLAYER_ID", "TEAM_ID", season_src]
    )

    team_lookup = (
        logs_min
          .groupby(["PLAYER_ID", season_src])["TEAM_ID"]
          .agg(lambda x: x.value_counts().idxmax())
          .reset_index()
          .rename(columns={season_src: "season", "TEAM_ID": "team_id"})
    )
    df = df.merge(team_lookup, on=["PLAYER_ID", "season"], how="left")
else:
    print(f"âš ï¸  {IN_LOGS.name} manquant â€“ team_id non ajoutÃ©")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  8.Â Sauvegarde
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"âœ…  Ã‰criture         : {OUTFILE.name}  ({len(df):,} lignes, {df.shape[1]}Â colonnes)")
df.to_parquet(OUTFILE, index=False)
