#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
dashboard_data.py
-----------------
Fusionne automatiquement :
 - dataset_ml.parquet       (features + target_note_n1)
 - wins_shares_vorp.parquet (Win Shares & VORP)
 - player_clusters.parquet  (clusters)
 - all_player_gamelogs.parquet (pour extraire tous les player_name)

et g√©n√®re `dashboard_data.parquet` pr√™t √† √™tre consomm√© par le dashboard.

Usage :
    python -m nba_rating.scripts.dashboard_data
"""

import pandas as pd
from pathlib import Path

def make_photo_url(player_id: int, size: str = "260x190") -> str:
    """
    Construit l'URL publique du headshot NBA pour un joueur donn√©.
    """
    return f"https://cdn.nba.com/headshots/nba/latest/{size}/{int(player_id)}.png"

# 1) D√©finition des chemins
BASE       = Path(__file__).resolve().parents[1] / "data"
CURATED    = BASE / "curated"
RAW        = BASE / "raw"
IN_DS      = CURATED / "dataset_ml.parquet"
IN_WS      = CURATED / "wins_shares_vorp.parquet"
IN_CL      = CURATED / "player_clusters.parquet"
IN_LOGS    = CURATED / "all_player_gamelogs.parquet"
OUT        = CURATED / "dashboard_data.parquet"

# 2) Chargement du dataset principal
print(f"üîÑ Chargement      : {IN_DS.name}")
df = pd.read_parquet(IN_DS)

# 2a) Enrichissement : rattacher poste et √©quipe pour chaque saison
from pathlib import Path
# Rassembler les informations de position et √©quipe depuis player_season files
season_files = sorted(Path(CURATED).glob("player_season_*.parquet"))
frames = []
for path in season_files:
    season = path.stem.split("_")[-1]
    ps = pd.read_parquet(path, columns=["PLAYER_ID", "POSITION", "TEAM_ID"])
    ps = ps.rename(columns={"POSITION": "position", "TEAM_ID": "team"})
    ps["season"] = season
    frames.append(ps)
pos_team_df = pd.concat(frames, ignore_index=True)
df = df.merge(pos_team_df, on=["PLAYER_ID", "season"], how="left")

# 2a) Fusion du score_100 depuis all_seasons_scores.parquet
SCORES_PATH = CURATED / "all_seasons_scores.parquet"
if SCORES_PATH.exists():
    print(f"üîÑ Fusion score_100 : {SCORES_PATH.name}")
    scores_df = (
        pd.read_parquet(SCORES_PATH)[["PLAYER_ID", "season", "score_100"]]
        .drop_duplicates(subset=["PLAYER_ID", "season"])
    )
    df = df.merge(scores_df, on=["PLAYER_ID", "season"], how="left")

else:
    print(f"‚ö†Ô∏è Fichier introuvable : {SCORES_PATH.name} (score_100 non ajout√©)")

# 2b) Normalisation du score_100
# Normalisation du score_100 issu de merges multiples
if "score_100_x" in df.columns and "score_100_y" in df.columns:
    df["score_100"] = df["score_100_x"].fillna(df["score_100_y"])
    df = df.drop(columns=["score_100_x", "score_100_y"])
elif "score_100_x" in df.columns:
    df = df.rename(columns={"score_100_x": "score_100"})
elif "score_100_y" in df.columns:
    df = df.rename(columns={"score_100_y": "score_100"})
    
# 3) Extraction des noms depuis all_player_gamelogs.parquet (curated)
LOGS_ALL = CURATED / "all_player_gamelogs.parquet"
if LOGS_ALL.exists():
    print(f"üîÑ Extraction des noms : {LOGS_ALL.name}")
    names = (
        pd.read_parquet(LOGS_ALL)[["PLAYER_ID", "PLAYER_NAME"]]
        .drop_duplicates(subset=["PLAYER_ID"])
        .rename(columns={"PLAYER_NAME": "player_name_new"})
    )
    # Conserver l'ancien nom si pr√©sent
    if "player_name" in df.columns:
        df = df.rename(columns={"player_name": "player_name_old"})
    # Fusionner les nouveaux noms
    df = df.merge(names, on="PLAYER_ID", how="left")
    # Combiner ancien et nouveau
    if "player_name_old" in df.columns:
        df["player_name"] = df["player_name_old"].fillna(df["player_name_new"])
    else:
        df["player_name"] = df["player_name_new"]
    # Supprimer colonnes interm√©diaires
    df = df.drop(columns=[c for c in ["player_name_old","player_name_new"] if c in df.columns])

    # 3) Construire l'URL de la photo headshot
    df["photo_url"] = df["PLAYER_ID"].apply(lambda pid: make_photo_url(pid))
else:
    print(f"‚ö†Ô∏è Fichier introuvable : {LOGS_ALL.name} ‚ûî noms non ajout√©s")

# 4) Fusion Win Shares / VORP
if IN_WS.exists():
    print(f"üîÑ Fusion WS/VORP  : {IN_WS.name}")
    ws = pd.read_parquet(IN_WS).drop_duplicates(subset=["PLAYER_ID","season"])
    df = df.merge(ws, on=["PLAYER_ID","season"], how="left")
else:
    print(f"‚ö†Ô∏è Fichier introuvable : {IN_WS.name}\n   Aucun Win Shares/VORP ajout√©")

# 5) Fusion Clusters
if IN_CL.exists():
    print(f"üîÑ Fusion clusters : {IN_CL.name}")
    cl = pd.read_parquet(IN_CL).drop_duplicates(subset=["PLAYER_ID","season"])
    # renommer si n√©cessaire
    if "player_cluster" in cl.columns:
        cl = cl.rename(columns={"player_cluster": "cluster"})
    df = df.merge(
        cl[["PLAYER_ID","season","cluster"]],
        on=["PLAYER_ID","season"],
        how="left"
    )
else:
    print(f"‚ö†Ô∏è Fichier introuvable : {IN_CL.name}\n   Aucun cluster ajout√©")

# 5a) Fusion des projections ML (predict_future)
PROJ_PATH = CURATED / "projections.parquet"
if PROJ_PATH.exists():
    print(f"üîÑ Fusion projections : {PROJ_PATH.name}")
    proj_df = pd.read_parquet(PROJ_PATH)
    # On s'attend √† colonnes ['PLAYER_ID','horizon','pred_score']
    # Pivot pour avoir une colonne par horizon
    proj_wide = proj_df.pivot(
        index="PLAYER_ID", 
        columns="horizon", 
        values="pred_score"
    ).add_prefix("proj_")
    proj_wide = proj_wide.reset_index()
    df = df.merge(proj_wide, on="PLAYER_ID", how="left")
else:
    print(f"‚ö†Ô∏è Fichier introuvable : {PROJ_PATH.name} ‚ûî projections non ajout√©es")

# 6) √âcriture du fichier final
print(f"‚úÖ √âcriture        : {OUT.name} ‚Üí {len(df)} lignes √ó {len(df.columns)} colonnes")
df.to_parquet(OUT, index=False)
