#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
generate_ws_vorp.py
-------------------
Automatisation de la collecte des m√©triques avanc√©es (Win Shares, VORP)
depuis Basketball-Reference pour chaque saison NBA de 1999-00 √† 2023-24,
puis matching sur PLAYER_ID via all_player_gamelogs.parquet, avec
normalisation des noms, fuzzy matching, et export des noms non mapp√©s.

Usage:
    python nba_rating/scripts/generate_ws_vorp.py
"""

import warnings
import unicodedata
import re
from pathlib import Path

import pandas as pd
from thefuzz import process


def normalize(name: str) -> str:
    if not isinstance(name, str):
        return ""
    # enlever accents
    nfkd = unicodedata.normalize("NFKD", name)
    ascii_str = "".join(c for c in nfkd if not unicodedata.combining(c))
    # ‚ûä remplacer tirets/traits d‚Äôunion par espace
    ascii_str = ascii_str.replace("-", " ").replace("‚Äì", " ")
    # ‚ûã minuscules, ne garder que alphanum & espace
    s = re.sub(r"[^a-zA-Z0-9 ]+", "", ascii_str.lower()).strip()
    # ‚ûå retirer suffixes
    s = re.sub(r"\b(jr|sr|ii|iii|iv)\b", "", s)
    # ‚ûç compresser espaces multiples
    return re.sub(r"\s+", " ", s).strip()


def fetch_season_table(year: int) -> pd.DataFrame:
    """
    R√©cup√®re la table "Advanced" pour la saison (year-1)-year depuis B-Ref,
    extrait Player, WS, VORP, ajoute 'season' format 'YYYY-YY'.
    """
    url = f"https://www.basketball-reference.com/leagues/NBA_{year}_advanced.html"
    try:
        df = pd.read_html(url)[0]
    except Exception as e:
        warnings.warn(f"Impossible de t√©l√©charger {url}: {e}")
        return pd.DataFrame()

    # flatten header
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = [
            col[1] if col[1] and not col[1].startswith("Unnamed") else col[0]
            for col in df.columns.values
        ]

    # supprimer lignes d'en-t√™te r√©p√©t√©es
    if "Player" in df.columns:
        df = df[df["Player"] != "Player"]

    # v√©rifier colonnes
    required = {"Player", "WS", "VORP"}
    found = set(df.columns)
    if not required.issubset(found):
        warnings.warn(
            f"Saison {year-1}-{str(year)[2:]} ignor√©e : attendues {required}, trouv√©es {found}"
        )
        return pd.DataFrame()

    df = df.loc[:, ["Player", "WS", "VORP"]].copy()
    df.columns = ["PLAYER_NAME", "Win_Shares", "VORP"]
    df["season"] = f"{year-1}-{str(year)[2:]}"
    return df


BASE        = Path(__file__).resolve().parents[1] / "data"
RAW_DIR     = BASE / "raw"
CURATED_DIR = BASE / "curated"
OUT_PATH    = CURATED_DIR / "wins_shares_vorp.parquet"
LOGS_UNI    = CURATED_DIR / "all_player_gamelogs.parquet"

if not LOGS_UNI.exists():
    raise FileNotFoundError("‚Ä¶lance d'abord rassemble_gamelogs.py‚Ä¶")

# 1) Scraping ‚Üí df_ws
years, dfs = range(2000,2025), []
for y in years:
    df_s = fetch_season_table(y)
    if not df_s.empty:
        dfs.append(df_s)
df_ws = pd.concat(dfs, ignore_index=True)

# 2) Normalisation
df_ws["PLAYER_NAME_NORM"] = df_ws["PLAYER_NAME"].apply(normalize)

# 3) Mapping complet
mapping = (
    pd.read_parquet(LOGS_UNI, columns=["PLAYER_ID","PLAYER_NAME"])
        .drop_duplicates("PLAYER_NAME")
)
mapping["PLAYER_NAME_NORM"] = mapping["PLAYER_NAME"].apply(normalize)

# 4) Merge norm
df_ext = df_ws.merge(mapping[["PLAYER_ID","PLAYER_NAME_NORM"]],
                        on="PLAYER_NAME_NORM", how="left")

# 5) Fuzzy matching
miss = df_ext["PLAYER_ID"].isna()
noms = df_ext.loc[miss,"PLAYER_NAME_NORM"].unique()
choices = mapping["PLAYER_NAME_NORM"].tolist()
fmap = {n: process.extractOne(n, choices, score_cutoff=90)[0]
        for n in noms if process.extractOne(n, choices, score_cutoff=90)}
df_ext["PLAYER_NAME_NORM_FUZZY"] = df_ext["PLAYER_NAME_NORM"].map(lambda x: fmap.get(x,x))
df_ext = df_ext.merge(mapping[["PLAYER_ID","PLAYER_NAME_NORM"]],
                        left_on="PLAYER_NAME_NORM_FUZZY",
                        right_on="PLAYER_NAME_NORM",
                        how="left",
                        suffixes=("","_fuzzy"))
df_ext["PLAYER_ID"] = df_ext["PLAYER_ID"].fillna(df_ext["PLAYER_ID_fuzzy"])

# 6) Manual map pour derniers cas
manual_map = {
    "league average": None,
    # ex :
    # "tre scott":       "762",
    # "jeenathan williams":"1630252",
    # ‚Ä¶
}
df_ext["PLAYER_NAME_NORM"] = df_ext["PLAYER_NAME_NORM"].replace(manual_map)
df_ext = df_ext.merge(mapping[["PLAYER_ID","PLAYER_NAME_NORM"]],
                        on="PLAYER_NAME_NORM",
                        how="left",
                        suffixes=("","_manu"))
df_ext["PLAYER_ID"] = df_ext["PLAYER_ID"].fillna(df_ext["PLAYER_ID_manu"])

# ‚Äî‚Äî DEBUG export non-mapp√©s ‚Äî‚Äî
unmatched = df_ext[df_ext["PLAYER_ID"].isna()]
if not unmatched.empty:
    to_debug = (unmatched[["PLAYER_NAME","PLAYER_NAME_NORM"]]
                .drop_duplicates())
    debug_path = CURATED_DIR / "unmatched_ws_vorp.parquet"
    to_debug.to_parquet(debug_path, index=False)
    print(f"üîç noms non mapp√©s ‚Üí {debug_path} ({len(to_debug)} uniq)")

# 7) Export final
df_ext.loc[:,["PLAYER_ID","season","Win_Shares","VORP"]] \
        .to_parquet(OUT_PATH, index=False)
print(f"‚úÖ wins_shares_vorp.parquet ({len(df_ext)}) ‚Üí {OUT_PATH}")